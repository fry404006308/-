# Vision Transformer 开始

# ViT 类

<div style='color:#fe618e;font-weight:800;font-size:23px;'>输入</div>

img.shape:

```python
torch.Size([1, 3, 256, 256])
```

<div style='color:#fe618e;font-weight:800;font-size:23px;'>patch_embedding后的shape</div>

:

```python
x = self.to_patch_embedding(img)
```

self.to_patch_embedding:

```python
Sequential(
  (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)
  (1): Linear(in_features=3072, out_features=1024, bias=True)
)
```

x.shape:

```python
torch.Size([1, 64, 1024])
```

<div style='color:#fe618e;font-weight:800;font-size:23px;'>cls_tokens</div>

:

```python
cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)
```

cls_tokens.shape:

```python
torch.Size([1, 1, 1024])
```

# Transformer 类

<div style='color:#fe618e;font-weight:800;font-size:23px;'>输入</div>

x.shape:

```python
torch.Size([1, 65, 1024])
```

## PreNorm 类

<div style='color:#fe618e;font-weight:800;font-size:23px;'>输入</div>

x.shape:

```python
torch.Size([1, 65, 1024])
```

### Attention 类

<div style='color:#fe618e;font-weight:800;font-size:23px;'>输入</div>

x.shape:

```python
torch.Size([1, 65, 1024])
```

<div style='color:#fd7949;font-weight:800;font-size:23px;'>输出</div>

ans.shape:

```python
torch.Size([1, 65, 1024])
```

### Attention 类 END

<div style='color:#fd7949;font-weight:800;font-size:23px;'>输出</div>

ans.shape:

```python
torch.Size([1, 65, 1024])
```

## PreNorm 类 END

## PreNorm 类

<div style='color:#fe618e;font-weight:800;font-size:23px;'>输入</div>

x.shape:

```python
torch.Size([1, 65, 1024])
```

### FeedForward 类

<div style='color:#fe618e;font-weight:800;font-size:23px;'>输入</div>

x.shape:

```python
torch.Size([1, 65, 1024])
```

self.net:

```python
Sequential(
  (0): Linear(in_features=1024, out_features=2048, bias=True)
  (1): GELU()
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=2048, out_features=1024, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)
```

<div style='color:#fd7949;font-weight:800;font-size:23px;'>输出</div>

ans.shape:

```python
torch.Size([1, 65, 1024])
```

### FeedForward 类 END

<div style='color:#fd7949;font-weight:800;font-size:23px;'>输出</div>

ans.shape:

```python
torch.Size([1, 65, 1024])
```

## PreNorm 类 END

<div style='color:#fd7949;font-weight:800;font-size:23px;'>输出</div>

x.shape:

```python
torch.Size([1, 65, 1024])
```

# Transformer 类 END

<div style='color:#3296ee;font-weight:800;font-size:23px;'>分类头</div>

self.mlp_head:

```python
Sequential(
  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (1): Linear(in_features=1024, out_features=4, bias=True)
)
```

<div style='color:#fd7949;font-weight:800;font-size:23px;'>输出</div>

ans.shape:

```python
torch.Size([1, 4])
```

# ViT 类 END

torch.Size([1, 4])
